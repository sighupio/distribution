# Copyright (c) 2017-present SIGHUP s.r.l All rights reserved.
# Use of this source code is governed by a BSD-style
# license that can be found in the LICENSE file.

name: qa
kind: pipeline
type: docker

node:
   performance: low

platform:
  os: linux
  arch: amd64

clone:
  depth: 1

environment:
  MISE_DATA_DIR: /mise-data

volumes:
  - name: mise-cache
    host:
      path: /root/mise_data_dir

steps:
  - name: install-tools
    image: quay.io/sighup/mise:v2025.4.4
    pull: always
    volumes:
      - name: mise-cache
        path: /mise-data
    commands:
      - mise install
      - eval "$(mise activate bash --shims)"

  - name: license-check
    image: quay.io/sighup/mise:v2025.4.4
    pull: always
    depends_on:
      - install-tools
    volumes:
      - name: mise-cache
        path: /mise-data
    commands:
      - eval "$(mise activate bash --shims)"
      - mise run license-check

  - name: schema-check
    image: quay.io/sighup/mise:v2025.4.4
    pull: always
    depends_on:
      - install-tools
    volumes:
      - name: mise-cache
        path: /mise-data
    commands:
      - eval "$(mise activate bash --shims)"
      - |
        cat schemas/public/ekscluster-kfd-v1alpha2.json | \
          json-patch -p schemas/private/ekscluster-kfd-v1alpha2.patch.json | \
          jq -r > /tmp/schemas-private-ekscluster-kfd-v1alpha2.json
      - diff schemas/private/ekscluster-kfd-v1alpha2.json /tmp/schemas-private-ekscluster-kfd-v1alpha2.json

  - name: test-schema
    image: quay.io/sighup/mise:v2025.4.4
    pull: always
    depends_on:
      - license-check
      - schema-check
    volumes:
      - name: mise-cache
        path: /mise-data
    commands:
      - eval "$(mise activate bash --shims)"
      - bats -t tests/e2e/kfddistribution/schema.sh

  - name: render
    image: quay.io/sighup/mise:v2025.4.4
    pull: always
    depends_on:
      - license-check
      - schema-check
    environment:
      NETRC_FILE:
        from_secret: NETRC_FILE
      FURYCTL_CONFIG: tests/e2e/kfddistribution/manifests/furyctl-init-cluster.yaml
      FURYCTL_DISTRO_LOCATION: ./
      FURYCTL_OUTDIR: ./
      FURYCTL_DISABLE_ANALYTICS: "true"
      KUBECONFIG: ./dummy
      KUBECTL_VERSION: "1.32.2"
    volumes:
      - name: mise-cache
        path: /mise-data
    commands:
      - |
        mise use kubectl@$${KUBECTL_VERSION}
        eval "$(mise activate bash --shims)"
      - echo $${NETRC_FILE} > /root/.netrc
      - furyctl download dependencies && furyctl dump template
      - mv distribution $${FURYTCL_OUTDIR}.furyctl/$$(yq .metadata.name $FURYCTL_CONFIG)
      - kustomize build $${FURYTCL_OUTDIR}.furyctl/$$(yq .metadata.name $FURYCTL_CONFIG)/distribution/manifests > distribution.yml

  - name: check-deprecated-apis
    image: us-docker.pkg.dev/fairwinds-ops/oss/pluto:v5
    pull: always
    depends_on:
      - render
    commands:
      # we use --ignore-deprecations because we don't want the CI to fail when the API has not been removed yet.
      - /pluto detect distribution.yml --ignore-deprecations --target-versions=k8s=v1.33.0

---
name: e2e-kfddistribution-1.33
kind: pipeline
type: docker

depends_on:
  - qa

clone:
  depth: 1

platform:
  os: linux
  arch: amd64

trigger:
  ref:
    include:
      - refs/tags/e2e-kfddistribution-**
      - refs/tags/e2e-all-**
      - refs/tags/v*
    exclude:
      - refs/tags/**-docs*

environment:
  MISE_DATA_DIR: /mise-data
  CLUSTER_NAME: ${DRONE_REPO_NAME}-${DRONE_BUILD_NUMBER}
  KUBECONFIG: /drone/src/kubeconfig
  CLUSTER_VERSION: v1.33.0

volumes:
  - name: dockersock
    host:
      path: /var/run/docker.sock
  - name: mise-cache
    host:
      path: /root/mise_data_dir

steps:
  - name: install-tools
    image: quay.io/sighup/mise:v2025.4.4
    pull: always
    volumes:
      - name: mise-cache
        path: /mise-data
    commands:
      - mise install
      - eval "$(mise activate bash --shims)"

  - name: create Kind cluster
    image: quay.io/sighup/mise:v2025.4.4
    pull: always
    depends_on:
      - install-tools
    volumes:
      - name: dockersock
        path: /var/run/docker.sock
      - name: mise-cache
        path: /mise-data
    commands:
      - eval "$(mise activate bash --shims)"
      - |
        cat <<EOF > kind-config.yaml
        kind: Cluster
        apiVersion: kind.x-k8s.io/v1alpha4
        networking:
          disableDefaultCNI: true
        nodes:
          - role: control-plane
          - role: worker
          - role: worker
        EOF
      - kind create cluster --name $${CLUSTER_NAME} --image registry.sighup.io/fury/kindest/node:$${CLUSTER_VERSION} --config kind-config.yaml
      - kind get kubeconfig --name $${CLUSTER_NAME} > $${KUBECONFIG}

  - name: e2e-kfddistribution
    image: quay.io/sighup/mise:v2025.4.4
    pull: always
    network_mode: host
    volumes:
      - name: mise-cache
        path: /mise-data
    depends_on: [create Kind cluster]
    commands:
      - eval "$(mise activate bash --shims)"
      - export KUBECONFIG=/drone/src/kubeconfig
      - until kubectl get serviceaccount default > /dev/null 2>&1; do echo "waiting for control-plane" && sleep 1; done
      - tests/e2e/kfddistribution/e2e-kfddistribution.sh

  - name: delete-kind-cluster
    image: quay.io/sighup/mise:v2025.4.4
    pull: always
    volumes:
      - name: dockersock
        path: /var/run/docker.sock
      - name: mise-cache
        path: /mise-data
    commands:
      - eval "$(mise activate bash --shims)"
      - kind delete cluster --name $${CLUSTER_NAME} || true
    depends_on:
      - e2e-kfddistribution
    when:
      status:
        - success
        - failure

---
# We test 1.32.0 -> 1.33.1 upgrade pipeline
name: e2e-kfddistribution-1.32.0-to-1.33.1
kind: pipeline
type: docker

depends_on:
  - qa

clone:
  depth: 1

platform:
  os: linux
  arch: amd64

trigger:
  ref:
    include:
      - refs/tags/e2e-kfddistribution-**
      - refs/tags/e2e-all-**
      - refs/tags/v*
    exclude:
      - refs/tags/**-docs*

environment:
  MISE_DATA_DIR: /mise-data
  CLUSTER_NAME: ${DRONE_REPO_NAME}-${DRONE_BUILD_NUMBER}-upgrades
  KUBECONFIG: /drone/src/kubeconfig-upgrades
  KUBECTL_VERSION: "1.32.2"
  CLUSTER_VERSION: v1.32.2

volumes:
  - name: dockersock
    host:
      path: /var/run/docker.sock
  - name: mise-cache
    host:
      path: /root/mise_data_dir

steps:
  - name: install-tools
    image: quay.io/sighup/mise:v2025.4.4
    pull: always
    volumes:
      - name: mise-cache
        path: /mise-data
    commands:
      - mise install
      - eval "$(mise activate bash --shims)"

  - name: create Kind cluster
    image: quay.io/sighup/mise:v2025.4.4
    pull: always
    depends_on:
      - install-tools
    volumes:
      - name: dockersock
        path: /var/run/docker.sock
      - name: mise-cache
        path: /mise-data
    commands:
      - |
        mise use kubectl@$${KUBECTL_VERSION}
        eval "$(mise activate bash --shims)"
      - |
        cat <<EOF > kind-config.yaml
        kind: Cluster
        apiVersion: kind.x-k8s.io/v1alpha4
        networking:
          disableDefaultCNI: true
        nodes:
          - role: control-plane
          - role: worker
          - role: worker
        EOF
      - kind create cluster --name $${CLUSTER_NAME} --image registry.sighup.io/fury/kindest/node:$${CLUSTER_VERSION} --config kind-config.yaml
      - kind get kubeconfig --name $${CLUSTER_NAME} > $${KUBECONFIG}

  - name: e2e-kfddistribution
    image: quay.io/sighup/mise:v2025.4.4
    pull: always
    network_mode: host
    volumes:
      - name: mise-cache
        path: /mise-data
    depends_on: [create Kind cluster]
    commands:
      - |
        mise use kubectl@$${KUBECTL_VERSION}
        eval "$(mise activate bash --shims)"
      - export KUBECONFIG=/drone/src/kubeconfig-upgrades
      - until kubectl get serviceaccount default > /dev/null 2>&1; do echo "waiting for control-plane" && sleep 1; done
      - tests/e2e/kfddistribution/e2e-kfddistribution-upgrades.sh

  - name: delete-kind-cluster
    image: quay.io/sighup/mise:v2025.4.4
    pull: always
    volumes:
      - name: dockersock
        path: /var/run/docker.sock
      - name: mise-cache
        path: /mise-data
    commands:
      - eval "$(mise activate bash --shims)"
      - kind delete cluster --name $${CLUSTER_NAME} || true
    depends_on:
      - e2e-kfddistribution
    when:
      status:
        - success
        - failure

---
# e2e tests are failing on CI: race conditions on the applications of some (apparently) unrelated CRDs.
# Manual testing (running the same pipeline locally macOS M1 Pro, with drone exec) results in a successful run.
name: e2e-ekscluster-1.33.1
kind: pipeline
type: docker

depends_on:
  - qa

clone:
  depth: 1

trigger:
  ref:
    include:
      - refs/tags/e2e-eks-**
      - refs/tags/e2e-all-**
      - refs/tags/v*

environment:
  MISE_DATA_DIR: /mise-data
  DISTRIBUTION_VERSION: "v1.33.1"
  CLUSTER_NAME: e2e-eks-${DRONE_BUILD_NUMBER}

volumes:
  - name: mise-cache
    host:
      path: /root/mise_data_dir
  - name: furyctl-outdir
    temp: {}

steps:
  - name: install-tools
    image: quay.io/sighup/mise:v2025.4.4
    pull: always
    volumes:
      - name: mise-cache
        path: /mise-data
    commands:
      - mise install
      - eval "$(mise activate bash --shims)"

  - name: run-tests
    image: quay.io/sighup/mise:v2025.4.4
    pull: always
    environment:
      AWS_ACCESS_KEY_ID:
        from_secret: aws_access_key_id
      AWS_SECRET_ACCESS_KEY:
        from_secret: aws_secret_access_key
      AWS_REGION:
        from_secret: aws_region
    volumes:
      - name: mise-cache
        path: /mise-data
      - name: furyctl-outdir
        path: /furyctl-outdir
    commands:
      - eval "$(mise activate bash --shims)"
      - ./tests/e2e/ekscluster/e2e-ekscluster.sh
  
  - name: print-kubeconfig
    image: quay.io/sighup/mise:v2025.4.4
    pull: always
    environment:
      AWS_ACCESS_KEY_ID:
        from_secret: aws_access_key_id
      AWS_SECRET_ACCESS_KEY:
        from_secret: aws_secret_access_key
      AWS_REGION:
        from_secret: aws_region
    volumes:
      - name: mise-cache
        path: /mise-data
      - name: furyctl-outdir
        path: /furyctl-outdir
    commands:
      - echo "Tests failed, kubeconfig for investigation:"
      - cat /drone/src/kubeconfig
    when:
      status:
        - failure

  - name: furyctl-delete
    image: quay.io/sighup/mise:v2025.4.4
    pull: always
    environment:
      AWS_ACCESS_KEY_ID:
        from_secret: aws_access_key_id
      AWS_SECRET_ACCESS_KEY:
        from_secret: aws_secret_access_key
      AWS_REGION:
        from_secret: aws_region
    volumes:
      - name: mise-cache
        path: /mise-data
      - name: furyctl-outdir
        path: /furyctl-outdir
    commands:
      - eval "$(mise activate bash --shims)"
      - |
        furyctl delete cluster \
          --outdir /furyctl-outdir \
          --debug \
          --disable-analytics \
          --config "$(cat ./last_furyctl_yaml.txt)" \
          --distro-location ./ \
          --force \
          --skip-vpn-confirmation \
          --no-tty || true
    when:
      status:
        - success

  - name: furyctl-delete-force
    image: quay.io/sighup/mise:v2025.4.4
    pull: always
    environment:
      AWS_ACCESS_KEY_ID:
        from_secret: aws_access_key_id
      AWS_SECRET_ACCESS_KEY:
        from_secret: aws_secret_access_key
      AWS_REGION:
        from_secret: aws_region
    volumes:
      - name: mise-cache
        path: /mise-data
      - name: furyctl-outdir
        path: /furyctl-outdir
    commands:
      - eval "$(mise activate bash --shims)"
      - tests/e2e/ekscluster/replace_variables.sh --cluster-name "$CLUSTER_NAME" --furyctl-yaml tests/e2e/ekscluster/manifests/furyctl-cleanup-all.yaml
      - |
        furyctl delete cluster \
          --outdir /furyctl-outdir \
          --debug \
          --disable-analytics \
          --config tests/e2e/ekscluster/manifests/furyctl-cleanup-all.yaml \
          --distro-location ./ \
          --phase kubernetes \
          --force \
          --skip-vpn-confirmation \
          --no-tty || true
        furyctl delete cluster \
          --outdir /furyctl-outdir \
          --debug \
          --disable-analytics \
          --config tests/e2e/ekscluster/manifests/furyctl-cleanup-all.yaml \
          --distro-location ./ \
          --phase infrastructure \
          --force \
          --skip-vpn-confirmation \
          --no-tty || true
    when:
      status:
        - success

  - name: delete-aws-bucket
    image: quay.io/sighup/mise:v2025.4.4
    pull: always
    environment:
      AWS_ACCESS_KEY_ID:
        from_secret: aws_access_key_id
      AWS_SECRET_ACCESS_KEY:
        from_secret: aws_secret_access_key
      AWS_REGION:
        from_secret: aws_region
    volumes:
      - name: mise-cache
        path: /mise-data
      - name: furyctl-outdir
        path: /furyctl-outdir
    commands:
      - eval "$(mise activate bash --shims)"
      - aws s3 rm s3://e2e-drone-eks/$CLUSTER_NAME --recursive
    when:
      status:
        - success

---
name: e2e-ekscluster-selfmanaged-alinux2023-1.33.1
kind: pipeline
type: docker

depends_on:
  - qa

clone:
  depth: 1

trigger:
  ref:
    include:
      - refs/tags/e2e-eks-**
      - refs/tags/e2e-all-**
    exclude:
      - refs/tags/v*

environment:
  MISE_DATA_DIR: /mise-data
  DISTRIBUTION_VERSION: "v1.33.1"
  CLUSTER_NAME: e2e-eks-${DRONE_BUILD_NUMBER}-selfmanaged

volumes:
  - name: mise-cache
    host:
      path: /root/mise_data_dir
  - name: furyctl-outdir
    temp: {}

steps:
  - name: install-tools
    image: quay.io/sighup/mise:v2025.4.4
    pull: always
    volumes:
      - name: mise-cache
        path: /mise-data
    commands:
      - mise install
      - eval "$(mise activate bash --shims)"

  - name: run-tests
    image: quay.io/sighup/mise:v2025.4.4
    pull: always
    environment:
      AWS_ACCESS_KEY_ID:
        from_secret: aws_access_key_id
      AWS_SECRET_ACCESS_KEY:
        from_secret: aws_secret_access_key
      AWS_REGION:
        from_secret: aws_region
    volumes:
      - name: mise-cache
        path: /mise-data
      - name: furyctl-outdir
        path: /furyctl-outdir
    commands:
      - eval "$(mise activate bash --shims)"
      - ./tests/e2e/ekscluster/e2e-ekscluster-selfmanaged-alinux2023.sh

  - name: furyctl-delete
    image: quay.io/sighup/mise:v2025.4.4
    pull: always
    environment:
      AWS_ACCESS_KEY_ID:
        from_secret: aws_access_key_id
      AWS_SECRET_ACCESS_KEY:
        from_secret: aws_secret_access_key
      AWS_REGION:
        from_secret: aws_region
    volumes:
      - name: mise-cache
        path: /mise-data
      - name: furyctl-outdir
        path: /furyctl-outdir
    commands:
      - eval "$(mise activate bash --shims)"
      - |
        furyctl delete cluster \
          --outdir /furyctl-outdir \
          --debug \
          --disable-analytics \
          --config "$(cat ./last_furyctl_yaml.txt)" \
          --distro-location ./ \
          --force \
          --skip-vpn-confirmation \
          --no-tty
      - aws s3 rm s3://e2e-drone-eks/$CLUSTER_NAME --recursive

  - name: furyctl-delete-force
    image: quay.io/sighup/mise:v2025.4.4
    pull: always
    environment:
      AWS_ACCESS_KEY_ID:
        from_secret: aws_access_key_id
      AWS_SECRET_ACCESS_KEY:
        from_secret: aws_secret_access_key
      AWS_REGION:
        from_secret: aws_region
    volumes:
      - name: mise-cache
        path: /mise-data
      - name: furyctl-outdir
        path: /furyctl-outdir
    commands:
      - eval "$(mise activate bash --shims)"
      - tests/e2e/ekscluster/replace_variables.sh --cluster-name "$CLUSTER_NAME" --furyctl-yaml tests/e2e/ekscluster/manifests/furyctl-selfmanaged-alinux2023-init-cluster.yaml
      - |
        furyctl delete cluster \
          --outdir /furyctl-outdir \
          --debug \
          --disable-analytics \
          --config tests/e2e/ekscluster/manifests/furyctl-selfmanaged-alinux2023-init-cluster.yaml \
          --distro-location ./ \
          --phase kubernetes \
          --force \
          --skip-vpn-confirmation \
          --no-tty || true
        furyctl delete cluster \
          --outdir /furyctl-outdir \
          --debug \
          --disable-analytics \
          --config tests/e2e/ekscluster/manifests/furyctl-selfmanaged-alinux2023-init-cluster.yaml \
          --distro-location ./ \
          --phase infrastructure \
          --force \
          --skip-vpn-confirmation \
          --no-tty
      - aws s3 rm s3://e2e-drone-eks/$CLUSTER_NAME --recursive
    when:
      status:
        - failure

---
name: e2e-ekscluster-upgrades-1.32.0-1.33.1
kind: pipeline
type: docker

depends_on:
  - qa

clone:
  depth: 1

trigger:
  ref:
    include:
      - refs/tags/e2e-eks-**
      - refs/tags/e2e-all-**
      - refs/tags/v*

environment:
  MISE_DATA_DIR: /mise-data
  DISTRIBUTION_VERSION: "v1.33.1"
  CLUSTER_NAME: e2e-eks-${DRONE_BUILD_NUMBER}-upgrades

volumes:
  - name: mise-cache
    host:
      path: /root/mise_data_dir
  - name: furyctl-outdir
    temp: {}

steps:
  - name: install-tools
    image: quay.io/sighup/mise:v2025.4.4
    pull: always
    volumes:
      - name: mise-cache
        path: /mise-data
    commands:
      - mise install
      - eval "$(mise activate bash --shims)"

  - name: run-tests
    image: quay.io/sighup/mise:v2025.4.4
    pull: always
    environment:
      AWS_ACCESS_KEY_ID:
        from_secret: aws_access_key_id
      AWS_SECRET_ACCESS_KEY:
        from_secret: aws_secret_access_key
      AWS_REGION:
        from_secret: aws_region
    volumes:
      - name: mise-cache
        path: /mise-data
      - name: furyctl-outdir
        path: /furyctl-outdir
    commands:
      - eval "$(mise activate bash --shims)"
      - ./tests/e2e/ekscluster-upgrades/e2e-ekscluster-upgrades.sh

  - name: furyctl-delete
    image: quay.io/sighup/mise:v2025.4.4
    pull: always
    environment:
      AWS_ACCESS_KEY_ID:
        from_secret: aws_access_key_id
      AWS_SECRET_ACCESS_KEY:
        from_secret: aws_secret_access_key
      AWS_REGION:
        from_secret: aws_region
    volumes:
      - name: mise-cache
        path: /mise-data
      - name: furyctl-outdir
        path: /furyctl-outdir
    commands:
      - eval "$(mise activate bash --shims)"
      - |
        furyctl delete cluster \
          --outdir /furyctl-outdir \
          --debug \
          --disable-analytics \
          --config "$(cat ./last_furyctl_yaml.txt)" \
          --distro-location ./ \
          --force \
          --skip-vpn-confirmation \
          --no-tty
      - aws s3 rm s3://e2e-drone-eks/$CLUSTER_NAME --recursive

  - name: furyctl-delete-force
    image: quay.io/sighup/mise:v2025.4.4
    pull: always
    environment:
      AWS_ACCESS_KEY_ID:
        from_secret: aws_access_key_id
      AWS_SECRET_ACCESS_KEY:
        from_secret: aws_secret_access_key
      AWS_REGION:
        from_secret: aws_region
    volumes:
      - name: mise-cache
        path: /mise-data
      - name: furyctl-outdir
        path: /furyctl-outdir
    commands:
      - eval "$(mise activate bash --shims)"
      - tests/e2e/ekscluster/replace_variables.sh --cluster-name "$CLUSTER_NAME" --furyctl-yaml tests/e2e/ekscluster-upgrades/manifests/furyctl-upgrade-version-1.33.1.yaml
      - |
        furyctl delete cluster \
          --outdir /furyctl-outdir \
          --debug \
          --disable-analytics \
          --config tests/e2e/ekscluster-upgrades/manifests/furyctl-upgrade-version-1.33.1.yaml \
          --distro-location ./ \
          --phase kubernetes \
          --force \
          --skip-vpn-confirmation \
          --no-tty || true
        furyctl delete cluster \
          --outdir /furyctl-outdir \
          --debug \
          --disable-analytics \
          --config tests/e2e/ekscluster-upgrades/manifests/furyctl-upgrade-version-1.33.1.yaml \
          --distro-location ./ \
          --phase infrastructure \
          --force \
          --skip-vpn-confirmation \
          --no-tty
      - aws s3 rm s3://e2e-drone-eks/$CLUSTER_NAME --recursive
    when:
      status:
        - failure

---
name: e2e-onpremises
kind: pipeline
type: docker
node:
   performance: low

depends_on:
  - qa
clone:
  depth: 1

trigger:
  ref:
    include:
      - refs/tags/e2e-onpremises-**
      - refs/tags/e2e-all-**
      - refs/tags/v*

environment:
  MISE_DATA_DIR: /mise-data
  FURYCTL_VERSION: "v0.33.1-rc.1"
  TF_VAR_ci_number: ${DRONE_BUILD_NUMBER}

volumes:
  - name: cache
    temp: {}
  - name: mise-cache
    host:
      path: /root/mise_data_dir

steps:
  - name: install-tools
    image: quay.io/sighup/mise:v2025.4.4
    pull: always
    volumes:
      - name: mise-cache
        path: /mise-data
    commands:
      - mise install
      - eval "$(mise activate bash --shims)"

  - name: create-hetzner-infrastructure
    image: quay.io/sighup/mise:v2025.4.4
    pull: always
    environment:
      AWS_ACCESS_KEY_ID:
        from_secret: aws_access_key_id
      AWS_SECRET_ACCESS_KEY:
        from_secret: aws_secret_access_key
      AWS_REGION:
        from_secret: aws_region
      TF_VAR_hcloud_token:
        from_secret: hcloud_token
    volumes:
      - name: cache
        path: /cache
      - name: mise-cache
        path: /mise-data
    commands:
      - eval "$(mise activate bash --shims)"
      - cd tests/e2e/onpremises/
      - ssh-keygen -N '' -t rsa -b 4096 -C "sighup" -f /cache/ci-ssh-key
      - export TF_VAR_public_key=$(cat /cache/ci-ssh-key.pub)
      - export TF_VAR_private_key=$(cat /cache/ci-ssh-key)
      - ansible-playbook 0.local-tofu-run.yaml

  - name: install-cluster-with-furyctl
    image: quay.io/sighup/mise:v2025.4.4
    pull: always
    environment:
      AWS_ACCESS_KEY_ID:
        from_secret: aws_access_key_id
      AWS_SECRET_ACCESS_KEY:
        from_secret: aws_secret_access_key
      AWS_REGION:
        from_secret: aws_region
      TF_VAR_hcloud_token:
        from_secret: hcloud_token
    volumes:
      - name: cache
        path: /cache
      - name: mise-cache
        path: /mise-data
    commands:
      - eval "$(mise activate bash --shims)"
      - cd tests/e2e/onpremises/
      - ansible-playbook 1.remote-furyctl-exec.yaml

  - name: kube-bench-security-test
    image: quay.io/sighup/mise:v2025.4.4
    pull: always
    environment:
      AWS_ACCESS_KEY_ID:
        from_secret: aws_access_key_id
      AWS_SECRET_ACCESS_KEY:
        from_secret: aws_secret_access_key
      AWS_REGION:
        from_secret: aws_region
      TF_VAR_hcloud_token:
        from_secret: hcloud_token
    volumes:
      - name: cache
        path: /cache
      - name: mise-cache
        path: /mise-data
    commands:
      - eval "$(mise activate bash --shims)"
      - cd tests/e2e/onpremises/
      - ansible-playbook 2.remote-kube-bench-test.yaml

  - name: bats-tests
    image: quay.io/sighup/mise:v2025.4.4
    pull: always
    environment:
      KUBECONFIG: /cache/kubeconfig
    volumes:
      - name: cache
        path: /cache
      - name: mise-cache
        path: /mise-data
    commands:
      - eval "$(mise activate bash --shims)"
      - ./tests/e2e/onpremises/e2e-onpremises.sh

  - name: delete
    image: quay.io/sighup/mise:v2025.4.4
    pull: always
    environment:
      AWS_ACCESS_KEY_ID:
        from_secret: aws_access_key_id
      AWS_SECRET_ACCESS_KEY:
        from_secret: aws_secret_access_key
      AWS_REGION:
        from_secret: aws_region
      TF_VAR_hcloud_token:
        from_secret: hcloud_token
    volumes:
      - name: cache
        path: /cache
      - name: mise-cache
        path: /mise-data
    commands:
      - eval "$(mise activate bash --shims)"
      - cd tests/e2e/onpremises/
      - export TF_VAR_public_key=$(cat /cache/ci-ssh-key.pub)
      - export TF_VAR_private_key=$(cat /cache/ci-ssh-key)
      - ansible-playbook 3.local-tofu-destroy.yaml
    when:
      status:
        - success
        - failure

---
# On-premises upgrade pipeline - v1.32.0 to v1.33.1
name: e2e-onpremises-upgrades-1.32.0-1.33.1
kind: pipeline
type: docker

node:
   performance: low

depends_on:
  - qa

clone:
  depth: 1

trigger:
  ref:
    include:
      - refs/tags/e2e-onpremises-**
      - refs/tags/e2e-all-**
      - refs/tags/v*

environment:
  MISE_DATA_DIR: /mise-data
  FURYCTL_VERSION: "v0.33.1-rc.1"
  TF_VAR_ci_number: ${DRONE_BUILD_NUMBER}
  KUBECONFIG: /cache/kubeconfig

volumes:
  - name: cache-upgrades
    temp: {}
  - name: mise-cache
    host:
      path: /root/mise_data_dir

steps:
  - name: install-tools
    image: quay.io/sighup/mise:v2025.4.4
    pull: always
    volumes:
      - name: mise-cache
        path: /mise-data
    commands:
      - mise install
      - eval "$(mise activate bash --shims)"

  - name: create-hetzner-infrastructure
    image: quay.io/sighup/mise:v2025.4.4
    pull: always
    environment:
      DISTRIBUTION_VERSION: "v1.32.0"
      AWS_ACCESS_KEY_ID:
        from_secret: aws_access_key_id
      AWS_SECRET_ACCESS_KEY:
        from_secret: aws_secret_access_key
      AWS_REGION:
        from_secret: aws_region
      TF_VAR_hcloud_token:
        from_secret: hcloud_token
    volumes:
      - name: cache-upgrades
        path: /cache
      - name: mise-cache
        path: /mise-data
    commands:
      - eval "$(mise activate bash --shims)"
      - cd tests/e2e/onpremises-upgrades/
      - ssh-keygen -N '' -t rsa -b 4096 -C "sighup" -f /cache/ci-ssh-key
      - export TF_VAR_public_key=$(cat /cache/ci-ssh-key.pub)
      - export TF_VAR_private_key=$(cat /cache/ci-ssh-key)
      - ansible-playbook 0.local-tofu-run.yaml

  - name: install-cluster-1.32.0-with-furyctl
    image: quay.io/sighup/mise:v2025.4.4
    pull: always
    environment:
      DISTRIBUTION_VERSION: "v1.32.0" # distribution version is hardcoded in the tofu outputs
      AWS_ACCESS_KEY_ID:
        from_secret: aws_access_key_id
      AWS_SECRET_ACCESS_KEY:
        from_secret: aws_secret_access_key
      AWS_REGION:
        from_secret: aws_region
      TF_VAR_hcloud_token:
        from_secret: hcloud_token
    volumes:
      - name: cache-upgrades
        path: /cache
      - name: mise-cache
        path: /mise-data
    commands:
      - eval "$(mise activate bash --shims)"
      - cd tests/e2e/onpremises-upgrades/
      - ansible-playbook 1.remote-furyctl-exec.yaml

  - name: bats-tests
    image: quay.io/sighup/mise:v2025.4.4
    pull: always
    volumes:
      - name: cache-upgrades
        path: /cache
      - name: mise-cache
        path: /mise-data
    commands:
      - eval "$(mise activate bash --shims)"
      - ./tests/e2e/onpremises-upgrades/e2e-onpremises.sh

  - name: upgrade-cluster-to-1.33.1-with-furyctl
    image: quay.io/sighup/mise:v2025.4.4
    pull: always
    environment:
      DISTRIBUTION_VERSION: "v1.33.1" # distribution version is hardcoded in the tofu outputs
      AWS_ACCESS_KEY_ID:
        from_secret: aws_access_key_id
      AWS_SECRET_ACCESS_KEY:
        from_secret: aws_secret_access_key
      AWS_REGION:
        from_secret: aws_region
      TF_VAR_hcloud_token:
        from_secret: hcloud_token
    volumes:
      - name: cache-upgrades
        path: /cache
      - name: mise-cache
        path: /mise-data
    commands:
      - eval "$(mise activate bash --shims)"
      - cd tests/e2e/onpremises-upgrades/
      - ansible-playbook 2.remote-furyctl-exec-upgrade.yaml

  - name: bats-tests-upgrades
    image: quay.io/sighup/mise:v2025.4.4
    pull: always
    volumes:
      - name: cache-upgrades
        path: /cache
      - name: mise-cache
        path: /mise-data
    commands:
      - eval "$(mise activate bash --shims)"
      - ./tests/e2e/onpremises-upgrades/e2e-onpremises.sh

  - name: delete
    image: quay.io/sighup/mise:v2025.4.4
    pull: always
    environment:
      DISTRIBUTION_VERSION: "v1.33.1"
      AWS_ACCESS_KEY_ID:
        from_secret: aws_access_key_id
      AWS_SECRET_ACCESS_KEY:
        from_secret: aws_secret_access_key
      AWS_REGION:
        from_secret: aws_region
      TF_VAR_hcloud_token:
        from_secret: hcloud_token
    volumes:
      - name: cache-upgrades
        path: /cache
      - name: mise-cache
        path: /mise-data
    commands:
      - eval "$(mise activate bash --shims)"
      - cd tests/e2e/onpremises-upgrades/
      - export TF_VAR_public_key=$(cat /cache/ci-ssh-key.pub)
      - export TF_VAR_private_key=$(cat /cache/ci-ssh-key)
      - ansible-playbook 3.local-tofu-destroy.yaml
    when:
      status:
        - success
        - failure

---
kind: pipeline
type: docker
name: scheduled-aws-nuke

trigger:
  cron:
    - "scheduled-aws-nuke"

clone:
  depth: 1

steps:
  - name: run-aws-nuke
    image: amazon/aws-cli:latest
    environment:
      AWS_ACCESS_KEY_ID:
        from_secret: aws_access_key_id
      AWS_SECRET_ACCESS_KEY:
        from_secret: aws_secret_access_key
      AWS_DEFAULT_REGION: eu-west-1
      ACCOUNT_ID_PROD:
        from_secret: aws_account_id_prod
      AWS_NUKE_VERSION: "v3.51.1"
    commands:
      # Install required tools
      - yum update -y
      - yum install -y wget tar gzip jq
      # Install AWS Nuke
      - wget https://github.com/ekristen/aws-nuke/releases/download/$${AWS_NUKE_VERSION}/aws-nuke-$${AWS_NUKE_VERSION}-linux-amd64.tar.gz
      - tar -xvf aws-nuke-$${AWS_NUKE_VERSION}-linux-amd64.tar.gz
      - mv aws-nuke /usr/local/bin/aws-nuke
      - chmod +x /usr/local/bin/aws-nuke
      - export ACCOUNT_ID=$(aws sts get-caller-identity | jq -r ".Account")
      - sed "s/ACCOUNT_ID_PLACEHOLDER/$ACCOUNT_ID/" tests/e2e/ekscluster/awsnuke/config.yml > ./aws-nuke-config.yml
      - sed -i "s/ACCOUNT_ID_PROD_PLACEHOLDER/$ACCOUNT_ID_PROD/" ./aws-nuke-config.yml
      # Run AWS Nuke
      - aws-nuke nuke -c ./aws-nuke-config.yml --force --no-dry-run
      - aws s3 rm s3://e2e-drone-eks --recursive

---
name: release
kind: pipeline
type: docker

depends_on:
  - e2e-kfddistribution-1.33
  - e2e-kfddistribution-1.32.0-to-1.33.1
  - e2e-ekscluster-1.33.1
  - e2e-ekscluster-upgrades-1.32.0-1.33.1
  - e2e-onpremises
  - e2e-onpremises-upgrades-1.32.0-1.33.1

platform:
  os: linux
  arch: amd64

trigger:
  ref:
    include:
      - refs/tags/v**
    exclude:
      - refs/tags/**-docs*

steps:
  - name: prepare-release-notes
    image: quay.io/sighup/fury-release-notes-plugin:3.12_2.10.0
    depends_on: [clone]
    settings:
      release_notes_file_path: release-notes.md
    when:
      ref:
        include:
          - refs/tags/v**
        exclude:
          - refs/tags/**-docs*

  - name: publish-prerelease
    image: plugins/github-release
    pull: always
    depends_on:
      - prepare-release-notes
    settings:
      api_key:
        from_secret: github_token
      file_exists: skip
      files:
        - kfd.yaml
      prerelease: true
      overwrite: true
      title: "Prerelease ${DRONE_TAG}"
      note: release-notes.md
      checksum:
        - md5
        - sha256
    when:
      ref:
        include:
          - refs/tags/v**-rc**
        exclude:
          - refs/tags/**-docs*

  - name: publish-stable
    image: plugins/github-release
    pull: always
    depends_on:
      - prepare-release-notes
    settings:
      api_key:
        from_secret: github_token
      file_exists: skip
      files:
        - kfd.yaml
      prerelease: false
      overwrite: true
      title: "Release ${DRONE_TAG}"
      note: release-notes.md
      checksum:
        - md5
        - sha256
    when:
      ref:
        exclude:
          - refs/tags/v**-rc**
          - refs/tags/**-docs*
        include:
          - refs/tags/v**
